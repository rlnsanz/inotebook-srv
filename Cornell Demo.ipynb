{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "765e7dab-2419-4e76-bec6-1c1417e5914d",
   "metadata": {},
   "source": [
    "I would like to query the KVS above:\n",
    "1. Get the loss in a plotting format\n",
    "2. Get the accuracy in a plotting format\n",
    "3. Compose (You are now building a dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7444530e-5e3a-4d13-886a-360b1bfd9f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cce81ea9-6894-48e6-9236-043b017518ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>projid</th>\n",
       "      <th>vid</th>\n",
       "      <th>recrep</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cornelldbg</td>\n",
       "      <td>/home/vscode/.flor/cornelldbg/2022-01-30T20:14...</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>loss</td>\n",
       "      <td>2.335782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cornelldbg</td>\n",
       "      <td>/home/vscode/.flor/cornelldbg/2022-01-30T20:14...</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>loss</td>\n",
       "      <td>1.168365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cornelldbg</td>\n",
       "      <td>/home/vscode/.flor/cornelldbg/2022-01-30T20:14...</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>loss</td>\n",
       "      <td>1.119433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cornelldbg</td>\n",
       "      <td>/home/vscode/.flor/cornelldbg/2022-01-30T20:14...</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>loss</td>\n",
       "      <td>0.348132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cornelldbg</td>\n",
       "      <td>/home/vscode/.flor/cornelldbg/2022-01-30T20:14...</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>loss</td>\n",
       "      <td>0.37382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>cornelldbg</td>\n",
       "      <td>/home/vscode/.flor/cornelldbg/2022-01-30T20:14...</td>\n",
       "      <td>b</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>preds</td>\n",
       "      <td>/home/vscode/.flor/cornelldbg/data/dbf08fe5431...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>cornelldbg</td>\n",
       "      <td>/home/vscode/.flor/cornelldbg/2022-01-30T20:14...</td>\n",
       "      <td>b</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>test-batch</td>\n",
       "      <td>/home/vscode/.flor/cornelldbg/data/8d38968ce0e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>cornelldbg</td>\n",
       "      <td>/home/vscode/.flor/cornelldbg/2022-01-30T20:14...</td>\n",
       "      <td>b</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>preds</td>\n",
       "      <td>/home/vscode/.flor/cornelldbg/data/5fe46f770f1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>cornelldbg</td>\n",
       "      <td>/home/vscode/.flor/cornelldbg/2022-01-30T20:14...</td>\n",
       "      <td>b</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>test-batch</td>\n",
       "      <td>/home/vscode/.flor/cornelldbg/data/4a580df7ab5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>cornelldbg</td>\n",
       "      <td>/home/vscode/.flor/cornelldbg/2022-01-30T20:14...</td>\n",
       "      <td>b</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>preds</td>\n",
       "      <td>/home/vscode/.flor/cornelldbg/data/558bb3e9ddc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1428 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          projid                                                vid recrep  \\\n",
       "0     cornelldbg  /home/vscode/.flor/cornelldbg/2022-01-30T20:14...      a   \n",
       "1     cornelldbg  /home/vscode/.flor/cornelldbg/2022-01-30T20:14...      a   \n",
       "2     cornelldbg  /home/vscode/.flor/cornelldbg/2022-01-30T20:14...      a   \n",
       "3     cornelldbg  /home/vscode/.flor/cornelldbg/2022-01-30T20:14...      a   \n",
       "4     cornelldbg  /home/vscode/.flor/cornelldbg/2022-01-30T20:14...      a   \n",
       "...          ...                                                ...    ...   \n",
       "1423  cornelldbg  /home/vscode/.flor/cornelldbg/2022-01-30T20:14...      b   \n",
       "1424  cornelldbg  /home/vscode/.flor/cornelldbg/2022-01-30T20:14...      b   \n",
       "1425  cornelldbg  /home/vscode/.flor/cornelldbg/2022-01-30T20:14...      b   \n",
       "1426  cornelldbg  /home/vscode/.flor/cornelldbg/2022-01-30T20:14...      b   \n",
       "1427  cornelldbg  /home/vscode/.flor/cornelldbg/2022-01-30T20:14...      b   \n",
       "\n",
       "     epoch  step        name  \\\n",
       "0        1     0        loss   \n",
       "1        1     1        loss   \n",
       "2        1     2        loss   \n",
       "3        1     3        loss   \n",
       "4        1     4        loss   \n",
       "...    ...   ...         ...   \n",
       "1423    12     0       preds   \n",
       "1424    13     0  test-batch   \n",
       "1425    13     0       preds   \n",
       "1426    14     0  test-batch   \n",
       "1427    14     0       preds   \n",
       "\n",
       "                                                  value  \n",
       "0                                              2.335782  \n",
       "1                                              1.168365  \n",
       "2                                              1.119433  \n",
       "3                                              0.348132  \n",
       "4                                               0.37382  \n",
       "...                                                 ...  \n",
       "1423  /home/vscode/.flor/cornelldbg/data/dbf08fe5431...  \n",
       "1424  /home/vscode/.flor/cornelldbg/data/8d38968ce0e...  \n",
       "1425  /home/vscode/.flor/cornelldbg/data/5fe46f770f1...  \n",
       "1426  /home/vscode/.flor/cornelldbg/data/4a580df7ab5...  \n",
       "1427  /home/vscode/.flor/cornelldbg/data/558bb3e9ddc...  \n",
       "\n",
       "[1428 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df = flor.load_kvs()\n",
    "base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f61c912d-6c87-4d51-8a68-f8e3bb0553e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/vscode/.flor/cornelldbg/2022-01-30T20:14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/vscode/.flor/cornelldbg/2022-01-30T20:14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/vscode/.flor/cornelldbg/2022-01-30T20:14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/vscode/.flor/cornelldbg/2022-01-30T20:14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/vscode/.flor/cornelldbg/2022-01-30T20:14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>/home/vscode/.flor/cornelldbg/2022-01-30T20:14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>/home/vscode/.flor/cornelldbg/2022-01-30T20:14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>/home/vscode/.flor/cornelldbg/2022-01-30T20:14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>/home/vscode/.flor/cornelldbg/2022-01-30T20:14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>/home/vscode/.flor/cornelldbg/2022-01-30T20:14...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1428 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    vid\n",
       "0     /home/vscode/.flor/cornelldbg/2022-01-30T20:14...\n",
       "1     /home/vscode/.flor/cornelldbg/2022-01-30T20:14...\n",
       "2     /home/vscode/.flor/cornelldbg/2022-01-30T20:14...\n",
       "3     /home/vscode/.flor/cornelldbg/2022-01-30T20:14...\n",
       "4     /home/vscode/.flor/cornelldbg/2022-01-30T20:14...\n",
       "...                                                 ...\n",
       "1423  /home/vscode/.flor/cornelldbg/2022-01-30T20:14...\n",
       "1424  /home/vscode/.flor/cornelldbg/2022-01-30T20:14...\n",
       "1425  /home/vscode/.flor/cornelldbg/2022-01-30T20:14...\n",
       "1426  /home/vscode/.flor/cornelldbg/2022-01-30T20:14...\n",
       "1427  /home/vscode/.flor/cornelldbg/2022-01-30T20:14...\n",
       "\n",
       "[1428 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SELECT = ['vid']\n",
    "FROM = base_df\n",
    "WHERE = (base_df['projid'] == 'cornelldbg') \n",
    "\n",
    "q = FROM.loc[WHERE, SELECT]\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "feb08c17-427d-469e-8e1e-cb892ad813e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/vscode/.flor/cornelldbg/2022-01-30T20:14:43.json']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q['vid'].drop_duplicates().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ff065a9-fb76-4375-8724-d646ec310177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sh import tail\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67ea1d52-41f1-4239-82a6-04db9656a931",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread background thread for pid 5697:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/sh.py\", line 1683, in wrap\n",
      "    fn(*rgs, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/sh.py\", line 2662, in background_thread\n",
      "    handle_exit_code(exit_code)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/sh.py\", line 2349, in fn\n",
      "    return self.command.handle_command_exit_code(exit_code)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/sh.py\", line 905, in handle_command_exit_code\n",
      "    raise exc\n",
      "sh.ErrorReturnCode_1: \n",
      "\n",
      "  RAN: /usr/bin/tail -f /home/vscode/.flor/cornelldbg/2022-01-30T20:14:43.json\n",
      "\n",
      "  STDOUT:\n",
      "\n",
      "\n",
      "  STDERR:\n",
      "/usr/bin/tail: cannot open '/home/vscode/.flor/cornelldbg/2022-01-30T20:14:43.json' for reading: No such file or directory\n",
      "/usr/bin/tail: no files remaining\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ErrorReturnCode_1",
     "evalue": "\n\n  RAN: /usr/bin/tail -f /home/vscode/.flor/cornelldbg/2022-01-30T20:14:43.json\n\n  STDOUT:\n\n\n  STDERR:\n/usr/bin/tail: cannot open '/home/vscode/.flor/cornelldbg/2022-01-30T20:14:43.json' for reading: No such file or directory\n/usr/bin/tail: no files remaining\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mErrorReturnCode_1\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6b68b7230b07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'/home/vscode/.flor/cornelldbg/2022-01-30T20:14:43.json'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0meof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-f'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/sh.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    954\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopped_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/sh.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_command_exit_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 \u001b[0;31m# if an iterable command is using an instance of OProc for its stdin,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/sh.py\u001b[0m in \u001b[0;36mhandle_command_exit_code\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mran\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"truncate_exc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             )\n\u001b[0;32m--> 905\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mErrorReturnCode_1\u001b[0m: \n\n  RAN: /usr/bin/tail -f /home/vscode/.flor/cornelldbg/2022-01-30T20:14:43.json\n\n  STDOUT:\n\n\n  STDERR:\n/usr/bin/tail: cannot open '/home/vscode/.flor/cornelldbg/2022-01-30T20:14:43.json' for reading: No such file or directory\n/usr/bin/tail: no files remaining\n"
     ]
    }
   ],
   "source": [
    "ds = []\n",
    "for path in ['/home/vscode/.flor/cornelldbg/2022-01-30T20:14:43.json']:\n",
    "    eof = tail('-f', path, _iter=True).next()\n",
    "    ds.append(json.loads(eof))\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f60391c-6388-4817-9572-3655cbbc95d9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: switchml: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `switchml'\n",
      "/bin/bash: _moduleraw: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `_moduleraw'\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.295055\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.178652\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.822555\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.820248\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.578176\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.268009\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.541448\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.363624\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.270283\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.169761\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.344499\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.391161\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.231985\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.213651\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.150546\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.312034\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.160801\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.177337\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.078017\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.132242\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.104705\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.120086\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.181181\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.108004\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.142571\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.043699\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.090156\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.332673\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.067366\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.188226\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.054732\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.072625\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.349937\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.059544\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.137614\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.284477\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.087289\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.188485\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.153651\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.128432\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.054329\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.288842\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.029619\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.126083\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.082363\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.437455\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.117206\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.162492\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.219688\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.268606\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.136559\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.121364\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.086016\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.024875\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.114986\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.104964\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.230550\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.087690\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.073808\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.082123\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.087845\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.040940\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.139025\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.354346\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.116131\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.099407\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.144688\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.036750\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.049387\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.033015\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.109655\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.303787\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.183136\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.020709\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.045381\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.283341\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.041341\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.124338\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.109823\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.009509\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.236382\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.102701\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.156848\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.045108\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.241133\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.094848\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.013892\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.108771\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.113793\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.039390\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.251744\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.076465\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.181555\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.193167\n",
      "logging final batch /home/ubuntu/.flor/cornelldbg/data/f3b5c568b57148eb9471df9f07b60b60.pkl\n",
      "logging final predictions /home/ubuntu/.flor/cornelldbg/data/3eeadaee53344fe99bd9a8641243b5f3.pkl\n",
      "\n",
      "Test set: Average loss: 0.0438, Accuracy: 9859/10000 (99%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.156697\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.052507\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.142711\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.076634\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.040018\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.034573\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.160529\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.137801\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.115773\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.050621\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.220567\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.022285\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.109728\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.108182\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.102205\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.144714\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.085955\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.128805\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.010957\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.163828\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.039482\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.052577\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.187074\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.051350\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.032968\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.166434\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.165203\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.016704\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.184220\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.051011\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.017233\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.036150\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.086781\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.147177\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.018409\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.021840\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.064891\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.029758\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.020473\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.038817\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.066282\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.241758\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.032015\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.030797\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.016394\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.040298\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.076758\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.010958\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.042559\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.070400\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.108588\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.014313\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.044503\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.012126\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.063659\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.105755\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.042338\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.053841\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.086028\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.096132\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.017054\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.024241\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.105445\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.020524\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.042113\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.224406\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.015501\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.093345\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.021587\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.016364\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.051930\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.046930\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.044763\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.074279\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.026711\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.013080\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.010777\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.087572\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.048883\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.074123\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.019343\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.076500\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.023983\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.008128\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.015080\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.017332\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.052378\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.027692\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.128325\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.085675\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.121174\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.178389\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.023485\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.075640\n",
      "logging final batch /home/ubuntu/.flor/cornelldbg/data/a5061b7375dd4331a2ab36b65f04dafa.pkl\n",
      "logging final predictions /home/ubuntu/.flor/cornelldbg/data/e510bd32abe443c5abf98019400f8ffd.pkl\n",
      "\n",
      "Test set: Average loss: 0.0399, Accuracy: 9866/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.037468\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.083965\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.023370\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.066188\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.017435\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.095484\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.092098\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.142611\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.070710\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.015419\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.063727\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.018661\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.013608\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.034975\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.037048\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.061831\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.025212\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.001850\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.028591\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.022319\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.026453\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.011976\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.115188\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.020178\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.024876\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.086305\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.010031\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.065991\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.102785\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.003085\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.128343\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.083165\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.085166\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.005246\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.006196\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.020719\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.074422\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.258314\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.037300\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.073797\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.005207\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.012064\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.065682\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.044416\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.056241\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.060094\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.226078\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.028201\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.083458\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.073568\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.044775\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.016769\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.004901\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.067596\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.184228\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.044298\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.010563\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.199999\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.018171\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.066893\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.074212\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.055201\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.095762\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.007192\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.072066\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.039199\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.005288\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.015949\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.344542\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.083253\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.006916\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.033855\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.072635\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.045830\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.015391\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.025428\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.042711\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.030151\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.001033\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.001753\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.128807\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.062226\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.007908\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.036825\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.012438\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.022799\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.003898\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.035223\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.123814\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.002155\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.010773\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.024704\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.092944\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.003323\n",
      "logging final batch /home/ubuntu/.flor/cornelldbg/data/b0227a6e3bd44bedb25cb34ac20c04b4.pkl\n",
      "logging final predictions /home/ubuntu/.flor/cornelldbg/data/17dcb9adfe6c4aa2a46099dfba150eee.pkl\n",
      "\n",
      "Test set: Average loss: 0.0303, Accuracy: 9897/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.006766\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.003794\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.010194\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.032499\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.059925\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.023919\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.014843\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.016125\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.017062\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.051407\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.001712\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.075574\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.053092\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.020193\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.045627\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.049641\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.083178\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.017356\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.041493\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.003811\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.003453\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.101390\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.008732\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.003903\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.011253\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.115926\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.058898\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.013626\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.029809\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.033812\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.007574\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.030975\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.026244\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.047642\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.004157\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.009101\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.022157\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.007551\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.012581\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.005808\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.010449\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.064020\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.027610\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.009080\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.006789\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.069488\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.003797\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.023540\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.044055\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.029973\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.008635\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.016360\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.140912\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.007717\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.001945\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.012372\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.100893\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.002329\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.114834\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.135272\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.047990\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.043444\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.011336\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.013544\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.149210\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.023553\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.023812\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.058738\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.001590\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.123774\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.033042\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.124531\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.051735\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.006841\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.114280\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.004070\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.004506\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.005303\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.007364\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.001841\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.002979\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.012403\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.017249\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.019720\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.061316\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.019873\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.027170\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.017504\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.165311\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.000970\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.010390\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.008197\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.032640\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.008582\n",
      "logging final batch /home/ubuntu/.flor/cornelldbg/data/b55feb472fe14b38babaf46d33d24073.pkl\n",
      "logging final predictions /home/ubuntu/.flor/cornelldbg/data/d1b0a74bd741436cba2343f6a516d05c.pkl\n",
      "\n",
      "Test set: Average loss: 0.0296, Accuracy: 9896/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.086341\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.037568\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.065102\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.003703\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.051793\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.025197\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.017345\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.002640\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.034432\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.006537\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.103383\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.032285\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.001750\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.097031\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.019844\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.007618\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.001592\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.078318\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.012351\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.006221\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.007984\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.001976\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.005549\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.034723\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.020655\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.010338\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.013315\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.018180\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.003534\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.008879\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.026445\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.056271\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.001637\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.120765\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.012053\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.063142\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.006658\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.000259\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.001941\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.061699\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.004543\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.011444\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.005220\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.017221\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.001708\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.052134\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.026449\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.001964\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.040202\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.035522\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.018919\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.007827\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.116511\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.019200\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.009659\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.034105\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.243612\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.027190\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.006967\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.003828\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.043208\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.013897\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.004882\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.000960\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.017452\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.055046\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.115365\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.045380\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.020284\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.001665\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.019494\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.031211\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.070961\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.001717\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.031567\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.026011\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.009163\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.003281\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.118015\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.065217\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.086529\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.003500\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.050887\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.064159\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.015921\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.005269\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.103105\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.007779\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.008898\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.045392\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.005105\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.043475\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.062484\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.066323\n",
      "logging final batch /home/ubuntu/.flor/cornelldbg/data/a95635a99a50424da709e1b9952cc25a.pkl\n",
      "logging final predictions /home/ubuntu/.flor/cornelldbg/data/e099236898ce4fbda1054ed1878419ab.pkl\n",
      "\n",
      "Test set: Average loss: 0.0281, Accuracy: 9911/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.030437\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.035030\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.002931\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.047502\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.050547\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.076121\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.033912\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.004309\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.108015\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.012837\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.011869\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.002874\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.004513\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.048758\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.105430\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.065272\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.155982\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.001902\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.006339\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.003733\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.144443\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.010024\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.035975\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.031099\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.028968\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.034196\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.015671\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.009323\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.021901\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.120392\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.042664\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.027514\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.030475\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.148123\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.040352\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.008803\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.022078\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.006598\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.023280\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.011524\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.020048\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.038277\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.006906\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.008223\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.002052\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.004179\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.003072\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.058247\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.022970\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.003681\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.049181\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.053765\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.000630\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.006752\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.007915\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.020922\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.008944\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.025414\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.018062\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.017063\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.067540\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.032088\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.003312\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.007960\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.024684\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.022849\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.014581\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.004942\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.006280\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.005927\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.023535\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.010295\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.029145\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.026341\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.014461\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.004696\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.019583\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.007544\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.030409\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.011112\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.020916\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.018326\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.061339\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.167551\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.049186\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.017800\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.045229\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.018721\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.041904\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.006937\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.020037\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.045393\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.003920\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.004584\n",
      "logging final batch /home/ubuntu/.flor/cornelldbg/data/8d62a9275baf4c1c8f6d3260ac6865ca.pkl\n",
      "logging final predictions /home/ubuntu/.flor/cornelldbg/data/df79843d129b4956a9eb18df5a9e5655.pkl\n",
      "\n",
      "Test set: Average loss: 0.0285, Accuracy: 9907/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.003113\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.018079\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.014255\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.095368\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.011422\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.017047\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.001103\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.077168\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.005550\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.000428\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.005788\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.004342\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.061041\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.026957\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.001724\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.052965\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.006455\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.034501\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.165650\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.016132\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.004996\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.008573\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.008737\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.008217\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.029476\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.067096\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.030869\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.011081\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.088418\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.064011\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.007066\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.032664\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.031890\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.005247\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.025538\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.032314\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.077061\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.008209\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.004534\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.028994\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.004653\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.001908\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.159594\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.070377\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.029725\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.008942\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.071258\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.024169\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.076807\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.115826\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.003178\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.005316\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.099958\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.003149\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.011543\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.092037\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.009388\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.022311\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.018830\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.002074\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.390389\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.051773\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.022701\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.021156\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.004010\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.027763\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.002953\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.054347\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.078439\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.032865\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.036587\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.069755\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.017569\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.017772\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.015723\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.091372\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.007582\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.043865\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.021239\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.022827\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.081996\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.028695\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.013900\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.069862\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.008670\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.121286\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.084048\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.043788\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.023701\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.024104\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.001252\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.008560\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.019891\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.006091\n",
      "logging final batch /home/ubuntu/.flor/cornelldbg/data/473dda6c54f24280ae568455421ba680.pkl\n",
      "logging final predictions /home/ubuntu/.flor/cornelldbg/data/6ada423a4ae846ada9867cbde8b4c6d9.pkl\n",
      "\n",
      "Test set: Average loss: 0.0278, Accuracy: 9905/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.018116\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.050668\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.003747\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.022771\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.042811\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.080754\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.004007\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.039391\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.017984\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.002336\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.014182\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.008852\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.003040\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.005531\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.038930\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.022685\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.009689\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.019659\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.013381\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.003612\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.005617\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.008542\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.017553\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.112672\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.000192\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.004603\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.045335\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.005019\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.007788\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.025984\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.043475\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.014140\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.001982\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.001979\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.028454\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.010523\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.032155\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.081222\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.002116\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.007329\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.008766\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.009736\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.000956\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.005132\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.022575\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.041207\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.001695\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.001759\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.005649\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.041614\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.001572\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.004542\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.008003\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.001361\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.056901\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.004139\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.001274\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.017052\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.004311\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.021116\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.004689\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.147135\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.058342\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.007819\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.004398\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.011427\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.048491\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.005694\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.016762\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.056712\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.034282\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.003465\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.024659\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.003179\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.028122\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.055442\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.004577\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.005256\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.005579\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.034246\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.011970\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.004625\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.006667\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.006367\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.004096\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.005655\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.005770\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.005157\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.055321\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.006185\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.066087\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.024647\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.006385\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.008090\n",
      "logging final batch /home/ubuntu/.flor/cornelldbg/data/c6ed95f34f9c4d4fb9b626fdebc4bf6a.pkl\n",
      "logging final predictions /home/ubuntu/.flor/cornelldbg/data/cd8e4a0c7cd442dab52c8494d87338bc.pkl\n",
      "\n",
      "Test set: Average loss: 0.0263, Accuracy: 9912/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.007691\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.081225\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.001287\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.231719\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.141702\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.000684\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.009417\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.002596\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.012971\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.010761\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.092416\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.037609\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.004205\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.153713\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.042070\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.095481\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.000716\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.020614\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.009268\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.014100\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.000496\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.001573\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.006752\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.008228\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.027917\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.015672\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.042686\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.002480\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.009787\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.017907\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.005082\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.006620\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.003366\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.104929\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.009815\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.001453\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.010072\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.112246\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.051092\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.001523\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.002602\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.010765\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.019947\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.069704\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.099119\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.012953\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.012475\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.007482\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.093052\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.011114\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.003835\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.013181\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.141223\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.006387\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.000958\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.005698\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.024243\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.007474\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.016793\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.011198\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.038267\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.009842\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.001093\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.038002\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.004598\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.075470\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.009021\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.017727\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.039109\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.003151\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.003746\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.043497\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.063628\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.009140\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.002835\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.005073\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.111019\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.002192\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.008255\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.009486\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.004707\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.024996\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.073928\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.022805\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.029922\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.005467\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.005791\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.030265\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.015764\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.043844\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.001187\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.005970\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.219199\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.020963\n",
      "logging final batch /home/ubuntu/.flor/cornelldbg/data/b3afbed528a74c7082291b4fe2fa1149.pkl\n",
      "logging final predictions /home/ubuntu/.flor/cornelldbg/data/d5333e269c9944f68cfdba673739de27.pkl\n",
      "\n",
      "Test set: Average loss: 0.0273, Accuracy: 9920/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.001238\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.005862\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.046646\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.046509\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.010866\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.004469\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.001719\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.025684\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.014043\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.006105\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.011356\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.031926\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.068407\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.105229\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.006299\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.002276\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.012728\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.017651\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.191536\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.055835\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.038533\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.154512\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.011811\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.005538\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.001650\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.010690\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.008541\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.014575\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.034101\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.200609\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.004251\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.007072\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.011194\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.072437\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.067531\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.003738\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.007434\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.003000\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.089585\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.000271\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.017825\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.014003\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.067918\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.008417\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.017435\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.003262\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.018291\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.004722\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.000614\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.010294\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.018993\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.000625\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.006140\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.045927\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.036031\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.007961\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.007392\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.002816\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.164154\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.004697\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.018518\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.022664\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.015613\n",
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.026815\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.005451\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.048462\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.012184\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.043848\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.018766\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.006015\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.045135\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.003058\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.034061\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.008916\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.003501\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.074938\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.031096\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.041021\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.006722\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.003698\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.014961\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.079893\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.013791\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.013884\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.056025\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.002473\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.002765\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.055236\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.015425\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.037501\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.020859\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.032060\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.002869\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.009668\n",
      "logging final batch /home/ubuntu/.flor/cornelldbg/data/d017102253544bacae15ab0c987431dd.pkl\n",
      "logging final predictions /home/ubuntu/.flor/cornelldbg/data/3e2a915b0ac84a6b896a1778bc7163d6.pkl\n",
      "\n",
      "Test set: Average loss: 0.0257, Accuracy: 9920/10000 (99%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.027419\n",
      "Train Epoch: 11 [640/60000 (1%)]\tLoss: 0.014137\n",
      "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 0.005082\n",
      "Train Epoch: 11 [1920/60000 (3%)]\tLoss: 0.021966\n",
      "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.053382\n",
      "Train Epoch: 11 [3200/60000 (5%)]\tLoss: 0.008677\n",
      "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 0.028149\n",
      "Train Epoch: 11 [4480/60000 (7%)]\tLoss: 0.005873\n",
      "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.003020\n",
      "Train Epoch: 11 [5760/60000 (10%)]\tLoss: 0.000258\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.017599\n",
      "Train Epoch: 11 [7040/60000 (12%)]\tLoss: 0.041935\n",
      "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.011788\n",
      "Train Epoch: 11 [8320/60000 (14%)]\tLoss: 0.012899\n",
      "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 0.020529\n",
      "Train Epoch: 11 [9600/60000 (16%)]\tLoss: 0.000475\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.001939\n",
      "Train Epoch: 11 [10880/60000 (18%)]\tLoss: 0.002741\n",
      "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 0.059835\n",
      "Train Epoch: 11 [12160/60000 (20%)]\tLoss: 0.057570\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.077430\n",
      "Train Epoch: 11 [13440/60000 (22%)]\tLoss: 0.020283\n",
      "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 0.013752\n",
      "Train Epoch: 11 [14720/60000 (25%)]\tLoss: 0.003602\n",
      "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.003220\n",
      "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 0.158430\n",
      "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 0.000741\n",
      "Train Epoch: 11 [17280/60000 (29%)]\tLoss: 0.003662\n",
      "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.012127\n",
      "Train Epoch: 11 [18560/60000 (31%)]\tLoss: 0.003227\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.003522\n",
      "Train Epoch: 11 [19840/60000 (33%)]\tLoss: 0.006540\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.007675\n",
      "Train Epoch: 11 [21120/60000 (35%)]\tLoss: 0.025175\n",
      "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 0.038857\n",
      "Train Epoch: 11 [22400/60000 (37%)]\tLoss: 0.072211\n",
      "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.072677\n",
      "Train Epoch: 11 [23680/60000 (39%)]\tLoss: 0.068760\n",
      "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 0.103184\n",
      "Train Epoch: 11 [24960/60000 (42%)]\tLoss: 0.002213\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.015613\n",
      "Train Epoch: 11 [26240/60000 (44%)]\tLoss: 0.015041\n",
      "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 0.032481\n",
      "Train Epoch: 11 [27520/60000 (46%)]\tLoss: 0.024154\n",
      "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.002536\n",
      "Train Epoch: 11 [28800/60000 (48%)]\tLoss: 0.005760\n",
      "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 0.153488\n",
      "Train Epoch: 11 [30080/60000 (50%)]\tLoss: 0.009356\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.047309\n",
      "Train Epoch: 11 [31360/60000 (52%)]\tLoss: 0.004085\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.080903\n",
      "Train Epoch: 11 [32640/60000 (54%)]\tLoss: 0.028912\n",
      "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.053398\n",
      "Train Epoch: 11 [33920/60000 (57%)]\tLoss: 0.001813\n",
      "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 0.024626\n",
      "Train Epoch: 11 [35200/60000 (59%)]\tLoss: 0.002479\n",
      "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.011731\n",
      "Train Epoch: 11 [36480/60000 (61%)]\tLoss: 0.002186\n",
      "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 0.006463\n",
      "Train Epoch: 11 [37760/60000 (63%)]\tLoss: 0.074097\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.002394\n",
      "Train Epoch: 11 [39040/60000 (65%)]\tLoss: 0.026417\n",
      "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 0.008195\n",
      "Train Epoch: 11 [40320/60000 (67%)]\tLoss: 0.006190\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.003890\n",
      "Train Epoch: 11 [41600/60000 (69%)]\tLoss: 0.061041\n",
      "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 0.008726\n",
      "Train Epoch: 11 [42880/60000 (71%)]\tLoss: 0.006448\n",
      "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.009012\n",
      "Train Epoch: 11 [44160/60000 (74%)]\tLoss: 0.046946\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.035683\n",
      "Train Epoch: 11 [45440/60000 (76%)]\tLoss: 0.071477\n",
      "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.023973\n",
      "Train Epoch: 11 [46720/60000 (78%)]\tLoss: 0.013482\n",
      "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 0.028100\n",
      "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 0.009965\n",
      "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.015989\n",
      "Train Epoch: 11 [49280/60000 (82%)]\tLoss: 0.011746\n",
      "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 0.027629\n",
      "Train Epoch: 11 [50560/60000 (84%)]\tLoss: 0.022653\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.015507\n",
      "Train Epoch: 11 [51840/60000 (86%)]\tLoss: 0.015141\n",
      "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 0.003008\n",
      "Train Epoch: 11 [53120/60000 (88%)]\tLoss: 0.019800\n",
      "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 0.077713\n",
      "Train Epoch: 11 [54400/60000 (91%)]\tLoss: 0.037224\n",
      "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 0.147463\n",
      "Train Epoch: 11 [55680/60000 (93%)]\tLoss: 0.007361\n",
      "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.010361\n",
      "Train Epoch: 11 [56960/60000 (95%)]\tLoss: 0.008960\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.047564\n",
      "Train Epoch: 11 [58240/60000 (97%)]\tLoss: 0.006558\n",
      "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.022580\n",
      "Train Epoch: 11 [59520/60000 (99%)]\tLoss: 0.017193\n",
      "logging final batch /home/ubuntu/.flor/cornelldbg/data/8f6b327955994380b79617a455a6b6f0.pkl\n",
      "logging final predictions /home/ubuntu/.flor/cornelldbg/data/f443fbfce4b946be8e741603f9fb8425.pkl\n",
      "\n",
      "Test set: Average loss: 0.0266, Accuracy: 9913/10000 (99%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.020753\n",
      "Train Epoch: 12 [640/60000 (1%)]\tLoss: 0.008642\n",
      "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 0.127919\n",
      "Train Epoch: 12 [1920/60000 (3%)]\tLoss: 0.033028\n",
      "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.010922\n",
      "Train Epoch: 12 [3200/60000 (5%)]\tLoss: 0.003705\n",
      "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 0.004801\n",
      "Train Epoch: 12 [4480/60000 (7%)]\tLoss: 0.012315\n",
      "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.043450\n",
      "Train Epoch: 12 [5760/60000 (10%)]\tLoss: 0.014041\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.003538\n",
      "Train Epoch: 12 [7040/60000 (12%)]\tLoss: 0.017410\n",
      "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.001709\n",
      "Train Epoch: 12 [8320/60000 (14%)]\tLoss: 0.031870\n",
      "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 0.003871\n",
      "Train Epoch: 12 [9600/60000 (16%)]\tLoss: 0.172617\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.050950\n",
      "Train Epoch: 12 [10880/60000 (18%)]\tLoss: 0.007451\n",
      "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 0.001798\n",
      "Train Epoch: 12 [12160/60000 (20%)]\tLoss: 0.006110\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.003810\n",
      "Train Epoch: 12 [13440/60000 (22%)]\tLoss: 0.011652\n",
      "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 0.032191\n",
      "Train Epoch: 12 [14720/60000 (25%)]\tLoss: 0.004423\n",
      "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.010051\n",
      "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 0.004911\n",
      "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 0.069261\n",
      "Train Epoch: 12 [17280/60000 (29%)]\tLoss: 0.043831\n",
      "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.002977\n",
      "Train Epoch: 12 [18560/60000 (31%)]\tLoss: 0.002499\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.031699\n",
      "Train Epoch: 12 [19840/60000 (33%)]\tLoss: 0.139680\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.004487\n",
      "Train Epoch: 12 [21120/60000 (35%)]\tLoss: 0.000726\n",
      "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 0.005159\n",
      "Train Epoch: 12 [22400/60000 (37%)]\tLoss: 0.011632\n",
      "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.009118\n",
      "Train Epoch: 12 [23680/60000 (39%)]\tLoss: 0.018921\n",
      "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 0.002775\n",
      "Train Epoch: 12 [24960/60000 (42%)]\tLoss: 0.005092\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.001113\n",
      "Train Epoch: 12 [26240/60000 (44%)]\tLoss: 0.002353\n",
      "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 0.036980\n",
      "Train Epoch: 12 [27520/60000 (46%)]\tLoss: 0.022835\n",
      "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.105437\n",
      "Train Epoch: 12 [28800/60000 (48%)]\tLoss: 0.009957\n",
      "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 0.003053\n",
      "Train Epoch: 12 [30080/60000 (50%)]\tLoss: 0.035346\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.002238\n",
      "Train Epoch: 12 [31360/60000 (52%)]\tLoss: 0.053424\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.024092\n",
      "Train Epoch: 12 [32640/60000 (54%)]\tLoss: 0.096643\n",
      "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.009177\n",
      "Train Epoch: 12 [33920/60000 (57%)]\tLoss: 0.008267\n",
      "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 0.001880\n",
      "Train Epoch: 12 [35200/60000 (59%)]\tLoss: 0.003485\n",
      "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.020050\n",
      "Train Epoch: 12 [36480/60000 (61%)]\tLoss: 0.005134\n",
      "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 0.001970\n",
      "Train Epoch: 12 [37760/60000 (63%)]\tLoss: 0.004926\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.001533\n",
      "Train Epoch: 12 [39040/60000 (65%)]\tLoss: 0.016440\n",
      "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 0.000638\n",
      "Train Epoch: 12 [40320/60000 (67%)]\tLoss: 0.021939\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.026981\n",
      "Train Epoch: 12 [41600/60000 (69%)]\tLoss: 0.003822\n",
      "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 0.020930\n",
      "Train Epoch: 12 [42880/60000 (71%)]\tLoss: 0.000700\n",
      "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 0.054367\n",
      "Train Epoch: 12 [44160/60000 (74%)]\tLoss: 0.001633\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.006129\n",
      "Train Epoch: 12 [45440/60000 (76%)]\tLoss: 0.005144\n",
      "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.002783\n",
      "Train Epoch: 12 [46720/60000 (78%)]\tLoss: 0.041441\n",
      "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 0.002898\n",
      "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.001242\n",
      "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.021060\n",
      "Train Epoch: 12 [49280/60000 (82%)]\tLoss: 0.042688\n",
      "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 0.006047\n",
      "Train Epoch: 12 [50560/60000 (84%)]\tLoss: 0.077206\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.047626\n",
      "Train Epoch: 12 [51840/60000 (86%)]\tLoss: 0.075063\n",
      "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 0.009839\n",
      "Train Epoch: 12 [53120/60000 (88%)]\tLoss: 0.014438\n",
      "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 0.004669\n",
      "Train Epoch: 12 [54400/60000 (91%)]\tLoss: 0.006967\n",
      "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 0.012659\n",
      "Train Epoch: 12 [55680/60000 (93%)]\tLoss: 0.062712\n",
      "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.006167\n",
      "Train Epoch: 12 [56960/60000 (95%)]\tLoss: 0.012604\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.050742\n",
      "Train Epoch: 12 [58240/60000 (97%)]\tLoss: 0.038672\n",
      "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.032081\n",
      "Train Epoch: 12 [59520/60000 (99%)]\tLoss: 0.004983\n",
      "logging final batch /home/ubuntu/.flor/cornelldbg/data/bc940c4f8eea41959cf9945d4926d4c6.pkl\n",
      "logging final predictions /home/ubuntu/.flor/cornelldbg/data/c490d40f28324351971b451452e16d17.pkl\n",
      "\n",
      "Test set: Average loss: 0.0263, Accuracy: 9919/10000 (99%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.085237\n",
      "Train Epoch: 13 [640/60000 (1%)]\tLoss: 0.045052\n",
      "Train Epoch: 13 [1280/60000 (2%)]\tLoss: 0.002181\n",
      "Train Epoch: 13 [1920/60000 (3%)]\tLoss: 0.003531\n",
      "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.001596\n",
      "Train Epoch: 13 [3200/60000 (5%)]\tLoss: 0.042508\n",
      "Train Epoch: 13 [3840/60000 (6%)]\tLoss: 0.001789\n",
      "Train Epoch: 13 [4480/60000 (7%)]\tLoss: 0.004022\n",
      "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.000379\n",
      "Train Epoch: 13 [5760/60000 (10%)]\tLoss: 0.006979\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.006461\n",
      "Train Epoch: 13 [7040/60000 (12%)]\tLoss: 0.001292\n",
      "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.001962\n",
      "Train Epoch: 13 [8320/60000 (14%)]\tLoss: 0.034983\n",
      "Train Epoch: 13 [8960/60000 (15%)]\tLoss: 0.009207\n",
      "Train Epoch: 13 [9600/60000 (16%)]\tLoss: 0.002753\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.005790\n",
      "Train Epoch: 13 [10880/60000 (18%)]\tLoss: 0.044074\n",
      "Train Epoch: 13 [11520/60000 (19%)]\tLoss: 0.038233\n",
      "Train Epoch: 13 [12160/60000 (20%)]\tLoss: 0.032769\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.007681\n",
      "Train Epoch: 13 [13440/60000 (22%)]\tLoss: 0.099350\n",
      "Train Epoch: 13 [14080/60000 (23%)]\tLoss: 0.004532\n",
      "Train Epoch: 13 [14720/60000 (25%)]\tLoss: 0.000361\n",
      "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.024446\n",
      "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 0.011487\n",
      "Train Epoch: 13 [16640/60000 (28%)]\tLoss: 0.072507\n",
      "Train Epoch: 13 [17280/60000 (29%)]\tLoss: 0.028860\n",
      "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.018426\n",
      "Train Epoch: 13 [18560/60000 (31%)]\tLoss: 0.005217\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.116133\n",
      "Train Epoch: 13 [19840/60000 (33%)]\tLoss: 0.042856\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.002509\n",
      "Train Epoch: 13 [21120/60000 (35%)]\tLoss: 0.003680\n",
      "Train Epoch: 13 [21760/60000 (36%)]\tLoss: 0.000934\n",
      "Train Epoch: 13 [22400/60000 (37%)]\tLoss: 0.003503\n",
      "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.026743\n",
      "Train Epoch: 13 [23680/60000 (39%)]\tLoss: 0.016505\n",
      "Train Epoch: 13 [24320/60000 (41%)]\tLoss: 0.101506\n",
      "Train Epoch: 13 [24960/60000 (42%)]\tLoss: 0.003740\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.060278\n",
      "Train Epoch: 13 [26240/60000 (44%)]\tLoss: 0.012501\n",
      "Train Epoch: 13 [26880/60000 (45%)]\tLoss: 0.025512\n",
      "Train Epoch: 13 [27520/60000 (46%)]\tLoss: 0.001908\n",
      "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.022846\n",
      "Train Epoch: 13 [28800/60000 (48%)]\tLoss: 0.021980\n",
      "Train Epoch: 13 [29440/60000 (49%)]\tLoss: 0.017852\n",
      "Train Epoch: 13 [30080/60000 (50%)]\tLoss: 0.006638\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.001846\n",
      "Train Epoch: 13 [31360/60000 (52%)]\tLoss: 0.048511\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.005413\n",
      "Train Epoch: 13 [32640/60000 (54%)]\tLoss: 0.010859\n",
      "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.006597\n",
      "Train Epoch: 13 [33920/60000 (57%)]\tLoss: 0.004446\n",
      "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 0.059841\n",
      "Train Epoch: 13 [35200/60000 (59%)]\tLoss: 0.008987\n",
      "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.000633\n",
      "Train Epoch: 13 [36480/60000 (61%)]\tLoss: 0.021182\n",
      "Train Epoch: 13 [37120/60000 (62%)]\tLoss: 0.001724\n",
      "Train Epoch: 13 [37760/60000 (63%)]\tLoss: 0.003142\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.009125\n",
      "Train Epoch: 13 [39040/60000 (65%)]\tLoss: 0.028781\n",
      "Train Epoch: 13 [39680/60000 (66%)]\tLoss: 0.001081\n",
      "Train Epoch: 13 [40320/60000 (67%)]\tLoss: 0.005464\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.005101\n",
      "Train Epoch: 13 [41600/60000 (69%)]\tLoss: 0.028790\n",
      "Train Epoch: 13 [42240/60000 (70%)]\tLoss: 0.022451\n",
      "Train Epoch: 13 [42880/60000 (71%)]\tLoss: 0.001602\n",
      "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.000239\n",
      "Train Epoch: 13 [44160/60000 (74%)]\tLoss: 0.032536\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.005441\n",
      "Train Epoch: 13 [45440/60000 (76%)]\tLoss: 0.005171\n",
      "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.009239\n",
      "Train Epoch: 13 [46720/60000 (78%)]\tLoss: 0.011811\n",
      "Train Epoch: 13 [47360/60000 (79%)]\tLoss: 0.035840\n",
      "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 0.011566\n"
     ]
    }
   ],
   "source": [
    "!python cornell_demo.py --flor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b64e7f-616f-4fd2-8c54-a06de0dd67b0",
   "metadata": {},
   "source": [
    "### Now I want to see some tensorboard logging\n",
    "1. log the loss and show in dashboard // or matplotlib your own\n",
    "2. hindsight log error points and backpropagate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c610e8fb-209c-43c4-9492-672a0da2f58c",
   "metadata": {},
   "source": [
    "Let's first get error points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dfe0dc7-ae1a-4f84-8510-bbb031dcba8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: switchml: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `switchml'\n",
      "/bin/bash: _moduleraw: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `_moduleraw'\n",
      "'Cornell Demo.ipynb'   Untitled.ipynb\t requirements.txt\n",
      " README.md\t       cornell_demo.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64fbc202-2b43-4c80-aabe-1c092f32529b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cornell-demo'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flor.flags.NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9b4f219-2ceb-4b1f-baea-337ce1d6cd97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PurePosixPath('/home/ubuntu/.flor/cornell-demo/2022-01-29T04:10:36.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flor.flags.INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bd53064-c63d-43e1-961f-72a6b549b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = '1.loss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0af6596e-addd-4ddd-b3b5-0a61645e0ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(k.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85565d58-befa-45f9-a794-ddd2ba5d74ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loss'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'.'.join(k.split('.')[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccb9a168-e8d1-4642-832b-d428f0a90e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: switchml: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `switchml'\n",
      "/bin/bash: _moduleraw: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `_moduleraw'\n",
      "{\n",
      "    \"NAME\": \"cornell-demo\",\n",
      "    \"MEMO\": \"/home/ubuntu/.flor/cornell-demo/2022-01-29T04:10:36.json\",\n",
      "    \"KVS\": {\n",
      "        \"1.loss\": [\n",
      "            2.2979211807250977,\n",
      "            1.5010101795196533,\n",
      "            0.7534269094467163,\n",
      "            0.6996073722839355,\n",
      "            0.4779623746871948,\n",
      "            0.3883739411830902,\n",
      "            0.2952060401439667,\n",
      "            0.7479898929595947,\n",
      "            0.4859619140625,\n",
      "            0.3428019881248474,\n",
      "            0.36664673686027527,\n",
      "            0.24958665668964386,\n",
      "            0.1762516051530838,\n",
      "            0.19649513065814972,\n",
      "            0.29563406109809875,\n",
      "            0.19739750027656555,\n",
      "            0.18975891172885895,\n",
      "            0.18262414634227753,\n",
      "            0.15122422575950623,\n",
      "            0.24814832210540771,\n",
      "            0.3065580427646637,\n",
      "            0.08964045345783234,\n",
      "            0.12862378358840942,\n",
      "            0.29425710439682007,\n",
      "            0.21037735044956207,\n",
      "            0.0927058532834053,\n",
      "            0.22123102843761444,\n",
      "            0.3225749433040619,\n",
      "            0.16461484134197235,\n",
      "            0.18353843688964844,\n",
      "            0.20115937292575836,\n",
      "            0.10676197707653046,\n",
      "            0.12814772129058838,\n",
      "            0.1367109715938568,\n",
      "            0.15910063683986664,\n",
      "            0.12964609265327454,\n",
      "            0.04025949537754059,\n",
      "            0.07101733982563019,\n",
      "            0.08172257244586945,\n",
      "            0.04486437514424324,\n",
      "            0.12141580134630203,\n",
      "            0.13659465312957764,\n",
      "            0.06979379057884216,\n",
      "            0.14536015689373016,\n",
      "            0.10840131342411041,\n",
      "            0.017405040562152863,\n",
      "            0.07954894006252289,\n",
      "            0.09909652918577194,\n",
      "            0.07664622366428375,\n",
      "            0.09876161813735962,\n",
      "            0.1406375765800476,\n",
      "            0.1677943617105484,\n",
      "            0.06313049048185349,\n",
      "            0.17629602551460266,\n",
      "            0.2566506862640381,\n",
      "            0.12914635241031647,\n",
      "            0.042765624821186066,\n",
      "            0.07872109115123749,\n",
      "            0.014917816035449505,\n",
      "            0.05345790460705757,\n",
      "            0.12141972780227661,\n",
      "            0.05514517053961754,\n",
      "            0.028116153553128242,\n",
      "            0.029246846213936806,\n",
      "            0.10286911576986313,\n",
      "            0.07814604789018631,\n",
      "            0.05326168239116669,\n",
      "            0.06031948700547218,\n",
      "            0.18715094029903412,\n",
      "            0.12699273228645325,\n",
      "            0.03007935918867588,\n",
      "            0.09460026770830154,\n",
      "            0.08791579306125641,\n",
      "            0.0784311294555664,\n",
      "            0.025207780301570892,\n",
      "            0.11578734964132309,\n",
      "            0.21943874657154083,\n",
      "            0.1273123323917389,\n",
      "            0.08336541801691055,\n",
      "            0.24355466663837433,\n",
      "            0.09354199469089508,\n",
      "            0.05255729705095291,\n",
      "            0.017949916422367096,\n",
      "            0.047362860292196274,\n",
      "            0.07839734107255936,\n",
      "            0.09388554096221924,\n",
      "            0.1727752387523651,\n",
      "            0.05170995369553566,\n",
      "            0.059279683977365494,\n",
      "            0.17274706065654755,\n",
      "            0.010257869958877563,\n",
      "            0.1939743012189865,\n",
      "            0.04743529483675957,\n",
      "            0.1438407301902771\n",
      "        ],\n",
      "        \"1.acc\": [\n",
      "            98.17\n",
      "        ],\n",
      "        \"2.loss\": [\n",
      "            0.008497619070112705,\n",
      "            0.16449613869190216,\n",
      "            0.004219057038426399,\n",
      "            0.04024410992860794,\n",
      "            0.03256051242351532,\n",
      "            0.2050004005432129,\n",
      "            0.035285815596580505,\n",
      "            0.02071073278784752,\n",
      "            0.12427468597888947,\n",
      "            0.04523724690079689,\n",
      "            0.05200684815645218,\n",
      "            0.12138421088457108,\n",
      "            0.013367805629968643,\n",
      "            0.15035425126552582,\n",
      "            0.0906422883272171,\n",
      "            0.036353133618831635,\n",
      "            0.015608560293912888,\n",
      "            0.11851561814546585,\n",
      "            0.14650149643421173,\n",
      "            0.021370505914092064,\n",
      "            0.05828685685992241,\n",
      "            0.23045237362384796,\n",
      "            0.09123781323432922,\n",
      "            0.02404826506972313,\n",
      "            0.06065044179558754,\n",
      "            0.09399042278528214,\n",
      "            0.039770156145095825,\n",
      "            0.06229301542043686,\n",
      "            0.024005375802516937,\n",
      "            0.003916184883564711,\n",
      "            0.26957184076309204,\n",
      "            0.10947296023368835,\n",
      "            0.03449028357863426,\n",
      "            0.011472277343273163,\n",
      "            0.20717115700244904,\n",
      "            0.057316143065690994,\n",
      "            0.024828316643834114,\n",
      "            0.0628766268491745,\n",
      "            0.2501908242702484,\n",
      "            0.03168848529458046,\n",
      "            0.024188270792365074,\n",
      "            0.019622331485152245,\n",
      "            0.0520147904753685,\n",
      "            0.16031669080257416,\n",
      "            0.05748508498072624,\n",
      "            0.06516396999359131,\n",
      "            0.14058077335357666,\n",
      "            0.04298805445432663,\n",
      "            0.1282438337802887,\n",
      "            0.05033016949892044,\n",
      "            0.011072131805121899,\n",
      "            0.14105436205863953,\n",
      "            0.011953556910157204,\n",
      "            0.022807147353887558,\n",
      "            0.1363278031349182,\n",
      "            0.012083880603313446,\n",
      "            0.12824857234954834,\n",
      "            0.11833173781633377,\n",
      "            0.22464805841445923,\n",
      "            0.10481056571006775,\n",
      "            0.08046319335699081,\n",
      "            0.03670124709606171,\n",
      "            0.09344688057899475,\n",
      "            0.11752159148454666,\n",
      "            0.15117748081684113,\n",
      "            0.015418612398207188,\n",
      "            0.1382211446762085,\n",
      "            0.042970336973667145,\n",
      "            0.11643871665000916,\n",
      "            0.04234306886792183,\n",
      "            0.04569157585501671,\n",
      "            0.02060060016810894,\n",
      "            0.05497889965772629,\n",
      "            0.05071922391653061,\n",
      "            0.1459583342075348,\n",
      "            0.23933090269565582,\n",
      "            0.1002257838845253,\n",
      "            0.12021945416927338,\n",
      "            0.04470931738615036,\n",
      "            0.0962139219045639,\n",
      "            0.0252082422375679,\n",
      "            0.013765360228717327,\n",
      "            0.008317025378346443,\n",
      "            0.05897074565291405,\n",
      "            0.042695678770542145,\n",
      "            0.051636457443237305,\n",
      "            0.11600642651319504,\n",
      "            0.027369944378733635,\n",
      "            0.016405267640948296,\n",
      "            0.08353549242019653,\n",
      "            0.004215035121887922,\n",
      "            0.1350628137588501,\n",
      "            0.11487087607383728,\n",
      "            0.013992869295179844\n",
      "        ],\n",
      "        \"2.acc\": [\n",
      "            98.74\n",
      "        ],\n",
      "        \"3.loss\": [\n",
      "            0.047128599137067795,\n",
      "            0.06938183307647705,\n",
      "            0.027485527098178864,\n",
      "            0.03418722003698349,\n",
      "            0.0031436176504939795,\n",
      "            0.07542230188846588,\n",
      "            0.03398427739739418,\n",
      "            0.08277495205402374,\n",
      "            0.007235895376652479,\n",
      "            0.010571184568107128,\n",
      "            0.1512281894683838,\n",
      "            0.07632581144571304,\n",
      "            0.04804503545165062,\n",
      "            0.04231296852231026,\n",
      "            0.03678823634982109,\n",
      "            0.022833505645394325,\n",
      "            0.08194945752620697,\n",
      "            0.06872723251581192,\n",
      "            0.03545243293046951,\n",
      "            0.08301902562379837,\n",
      "            0.05360328033566475,\n",
      "            0.027316877618432045,\n",
      "            0.046453025192022324,\n",
      "            0.06407421082258224,\n",
      "            0.01361218560487032,\n",
      "            0.03056558407843113,\n",
      "            0.03768056631088257,\n",
      "            0.06659778952598572,\n",
      "            0.04456869512796402,\n",
      "            0.013492710888385773,\n",
      "            0.02785058319568634,\n",
      "            0.007377017289400101,\n",
      "            0.016988391056656837,\n",
      "            0.008581798523664474,\n",
      "            0.073851577937603,\n",
      "            0.009672761894762516,\n",
      "            0.06773517280817032,\n",
      "            0.0367995984852314,\n",
      "            0.3997521996498108,\n",
      "            0.18177033960819244,\n",
      "            0.0505584217607975,\n",
      "            0.12216002494096756,\n",
      "            0.004918989259749651,\n",
      "            0.00541697395965457,\n",
      "            0.23573420941829681,\n",
      "            0.015040270052850246,\n",
      "            0.027263857424259186,\n",
      "            0.014933941885828972,\n",
      "            0.07593012601137161,\n",
      "            0.07681688666343689,\n",
      "            0.01357680931687355,\n",
      "            0.013529613614082336,\n",
      "            0.0009550870745442808,\n",
      "            0.008953399024903774,\n",
      "            0.00290804379619658,\n",
      "            0.0648702085018158,\n",
      "            0.06482671201229095,\n",
      "            0.016839418560266495,\n",
      "            0.001924846787005663,\n",
      "            0.1407446265220642,\n",
      "            0.0061468021012842655,\n",
      "            0.07483815401792526,\n",
      "            0.14453433454036713,\n",
      "            0.0052499910816550255,\n",
      "            0.1278487592935562,\n",
      "            0.0378350131213665,\n",
      "            0.043479226529598236,\n",
      "            0.01469480898231268,\n",
      "            0.09725888818502426,\n",
      "            0.18831974267959595,\n",
      "            0.04534982517361641,\n",
      "            0.08638062328100204,\n",
      "            0.008762049488723278,\n",
      "            0.0043012117967009544,\n",
      "            0.022270990535616875,\n",
      "            0.009404588490724564,\n",
      "            0.028154784813523293,\n",
      "            0.06029132753610611,\n",
      "            0.0979602262377739,\n",
      "            0.1558057814836502,\n",
      "            0.06635290384292603,\n",
      "            0.05218876525759697,\n",
      "            0.04817917197942734,\n",
      "            0.07019398361444473,\n",
      "            0.07709509879350662,\n",
      "            0.0012224774109199643,\n",
      "            0.009380022995173931,\n",
      "            0.02792854607105255,\n",
      "            0.0642012432217598,\n",
      "            0.007906845770776272,\n",
      "            0.168522447347641,\n",
      "            0.09636632353067398,\n",
      "            0.009909206070005894,\n",
      "            0.06525377929210663\n",
      "        ],\n",
      "        \"3.acc\": [\n",
      "            98.98\n",
      "        ],\n",
      "        \"4.loss\": [\n",
      "            0.07786644250154495,\n",
      "            0.020472025498747826,\n",
      "            0.008069733157753944,\n",
      "            0.011838180013000965,\n",
      "            0.11412729322910309,\n",
      "            0.09957081079483032,\n",
      "            0.013149546459317207,\n",
      "            0.09108825027942657,\n",
      "            0.22814464569091797,\n",
      "            0.0544542632997036,\n",
      "            0.08066830039024353,\n",
      "            0.053985822945833206,\n",
      "            0.0293692909181118,\n",
      "            0.04379839450120926,\n",
      "            0.00877460092306137,\n",
      "            0.006700012367218733,\n",
      "            0.018949458375573158,\n",
      "            0.025495700538158417,\n",
      "            0.004782470408827066,\n",
      "            0.04946300759911537,\n",
      "            0.06675058603286743,\n",
      "            0.05706395208835602,\n",
      "            0.019966674968600273,\n",
      "            0.08089037984609604,\n",
      "            0.0010958557249978185,\n",
      "            0.09797852486371994,\n",
      "            0.02955929934978485,\n",
      "            0.005881481803953648,\n",
      "            0.012165676802396774,\n",
      "            0.12856300175189972,\n",
      "            0.006152763031423092,\n",
      "            0.0015542570035904646,\n",
      "            0.0856020599603653,\n",
      "            0.027242016047239304,\n",
      "            0.053533416241407394,\n",
      "            0.10653462260961533,\n",
      "            0.09243642538785934,\n",
      "            0.16330966353416443,\n",
      "            0.036248575896024704,\n",
      "            0.047546811401844025,\n",
      "            0.03779693692922592,\n",
      "            0.02216736041009426,\n",
      "            0.0038739051669836044,\n",
      "            0.03531653806567192,\n",
      "            0.015539168380200863,\n",
      "            0.04495745524764061,\n",
      "            0.020634829998016357,\n",
      "            0.08994919806718826,\n",
      "            0.04948800057172775,\n",
      "            0.06797457486391068,\n",
      "            0.005590944550931454,\n",
      "            0.128355473279953,\n",
      "            0.010484171099960804,\n",
      "            0.0175787340849638,\n",
      "            0.01422154437750578,\n",
      "            0.03892907500267029,\n",
      "            0.013980426825582981,\n",
      "            0.009998497553169727,\n",
      "            0.02660524658858776,\n",
      "            0.014881661161780357,\n",
      "            0.06416387110948563,\n",
      "            0.020794782787561417,\n",
      "            0.0961044430732727,\n",
      "            0.0864686593413353,\n",
      "            0.007804864551872015,\n",
      "            0.013144326396286488,\n",
      "            0.022493112832307816,\n",
      "            0.12215044349431992,\n",
      "            0.0657530352473259,\n",
      "            0.032041121274232864,\n",
      "            0.004077152814716101,\n",
      "            0.0055341944098472595,\n",
      "            0.017825329676270485,\n",
      "            0.036163557320833206,\n",
      "            0.04730430617928505,\n",
      "            0.009097691625356674,\n",
      "            0.03045426867902279,\n",
      "            0.0071577527560293674,\n",
      "            0.009256026707589626,\n",
      "            0.08468727767467499,\n",
      "            0.021094612777233124,\n",
      "            0.09568511694669724,\n",
      "            0.004607913549989462,\n",
      "            0.12171349674463272,\n",
      "            0.029153959825634956,\n",
      "            0.027810631319880486,\n",
      "            0.009453262202441692,\n",
      "            0.0037951208651065826,\n",
      "            0.02965146116912365,\n",
      "            0.06605449318885803,\n",
      "            0.01838926039636135,\n",
      "            0.009944243356585503,\n",
      "            0.01022270042449236,\n",
      "            0.025643112137913704\n",
      "        ],\n",
      "        \"4.acc\": [\n",
      "            98.95\n",
      "        ],\n",
      "        \"5.loss\": [\n",
      "            0.12283477932214737,\n",
      "            0.006281440611928701,\n",
      "            0.009137892164289951,\n",
      "            0.0074708531610667706,\n",
      "            0.044971756637096405,\n",
      "            0.0014466439606621861,\n",
      "            0.042551618069410324,\n",
      "            0.00040221723611466587,\n",
      "            0.011052205227315426,\n",
      "            0.05651697888970375,\n",
      "            0.04886481538414955,\n",
      "            0.0017361281206831336,\n",
      "            0.011875269003212452,\n",
      "            0.021839618682861328,\n",
      "            0.030824847519397736,\n",
      "            0.21347250044345856,\n",
      "            0.0027158414013683796,\n",
      "            0.011449381709098816,\n",
      "            0.021858148276805878,\n",
      "            0.0147325424477458,\n",
      "            0.0929974839091301,\n",
      "            0.08661193400621414,\n",
      "            0.043451666831970215,\n",
      "            0.005427968688309193,\n",
      "            0.002571216318756342,\n",
      "            0.01856638491153717,\n",
      "            0.08501093834638596,\n",
      "            0.015908878296613693,\n",
      "            0.0009316687355749309,\n",
      "            0.0254677627235651,\n",
      "            0.0038767200894653797,\n",
      "            0.001164587796665728,\n",
      "            0.17956045269966125,\n",
      "            0.023879889398813248,\n",
      "            0.008656035177409649,\n",
      "            0.003485412336885929,\n",
      "            0.013726701959967613,\n",
      "            0.04502587765455246,\n",
      "            0.011489933356642723,\n",
      "            0.0816739872097969,\n",
      "            0.01897806115448475,\n",
      "            0.023848576471209526,\n",
      "            0.024867016822099686,\n",
      "            0.009339728392660618,\n",
      "            0.053815506398677826,\n",
      "            0.052097175270318985,\n",
      "            0.056388746947050095,\n",
      "            0.01666460745036602,\n",
      "            0.00022212740441318601,\n",
      "            0.001909087412059307,\n",
      "            0.0003288429870735854,\n",
      "            0.14527766406536102,\n",
      "            0.009686615318059921,\n",
      "            0.04807445779442787,\n",
      "            0.11775211244821548,\n",
      "            0.011637736111879349,\n",
      "            0.01622575707733631,\n",
      "            0.013115474954247475,\n",
      "            0.005649153608828783,\n",
      "            0.07876109331846237,\n",
      "            0.024748746305704117,\n",
      "            0.03297009319067001,\n",
      "            0.026639655232429504,\n",
      "            0.05507158115506172,\n",
      "            0.046325281262397766,\n",
      "            0.0033425758592784405,\n",
      "            0.0009337715455330908,\n",
      "            0.07571480423212051,\n",
      "            0.005686761345714331,\n",
      "            0.027773035690188408,\n",
      "            0.014970749616622925,\n",
      "            0.03774695470929146,\n",
      "            0.0014372379519045353,\n",
      "            0.055628109723329544,\n",
      "            0.0003613023436628282,\n",
      "            0.0034979982301592827,\n",
      "            0.1444956660270691,\n",
      "            0.015571869909763336,\n",
      "            0.12590225040912628,\n",
      "            0.012001062743365765,\n",
      "            0.004744475241750479,\n",
      "            0.03683243319392204,\n",
      "            0.005757968872785568,\n",
      "            0.020104818046092987,\n",
      "            0.13473546504974365,\n",
      "            0.009841160848736763,\n",
      "            0.009033739566802979,\n",
      "            0.09278088808059692,\n",
      "            0.032847851514816284,\n",
      "            0.2037610113620758,\n",
      "            0.07432369887828827,\n",
      "            0.004289206583052874,\n",
      "            0.09775076806545258,\n",
      "            0.0041945013217628\n",
      "        ],\n",
      "        \"5.acc\": [\n",
      "            99.12\n",
      "        ],\n",
      "        \"6.loss\": [\n",
      "            0.002211286686360836,\n",
      "            0.07844985276460648,\n",
      "            0.012394862249493599,\n",
      "            0.041175153106451035,\n",
      "            0.1428762823343277,\n",
      "            0.04753176122903824,\n",
      "            0.00036326050758361816,\n",
      "            0.04971078783273697,\n",
      "            0.03399096801877022,\n",
      "            0.02594447135925293,\n",
      "            0.06455354392528534,\n",
      "            0.08152464777231216,\n",
      "            0.0033964947797358036,\n",
      "            0.0018506103660911322,\n",
      "            0.0297715924680233,\n",
      "            0.00520486431196332,\n",
      "            0.009657121263444424,\n",
      "            0.018864857032895088,\n",
      "            0.008563482202589512,\n",
      "            0.01081243995577097,\n",
      "            0.0026519480161368847,\n",
      "            0.0021377354860305786,\n",
      "            0.004351369105279446,\n",
      "            0.03438146412372589,\n",
      "            0.00935936439782381,\n",
      "            0.026139963418245316,\n",
      "            0.022973597049713135,\n",
      "            0.010698946192860603,\n",
      "            0.06098782271146774,\n",
      "            0.01471546571701765,\n",
      "            0.0014094577636569738,\n",
      "            0.008007683791220188,\n",
      "            0.08127845823764801,\n",
      "            0.008081136271357536,\n",
      "            0.119514599442482,\n",
      "            0.07179684937000275,\n",
      "            0.05447288602590561,\n",
      "            0.02600240148603916,\n",
      "            0.0022925222292542458,\n",
      "            0.0033107586205005646,\n",
      "            0.01011494267731905,\n",
      "            0.0221019946038723,\n",
      "            0.023044638335704803,\n",
      "            0.0863739624619484,\n",
      "            0.04770819470286369,\n",
      "            0.0025380791630595922,\n",
      "            0.06553638726472855,\n",
      "            0.010225517675280571,\n",
      "            0.11333077400922775,\n",
      "            0.008410154841840267,\n",
      "            0.000742861710023135,\n",
      "            0.07983656972646713,\n",
      "            0.002593873767182231,\n",
      "            0.00900568999350071,\n",
      "            0.0064165666699409485,\n",
      "            0.00905910599976778,\n",
      "            0.0006023308960720897,\n",
      "            0.024398308247327805,\n",
      "            0.08268320560455322,\n",
      "            0.01563197374343872,\n",
      "            0.04637225717306137,\n",
      "            0.15573444962501526,\n",
      "            0.004468666855245829,\n",
      "            0.006584789603948593,\n",
      "            0.0065663158893585205,\n",
      "            0.017833644524216652,\n",
      "            0.011217688210308552,\n",
      "            0.019595760852098465,\n",
      "            0.06820454448461533,\n",
      "            0.10447119176387787,\n",
      "            0.00327822333201766,\n",
      "            0.08447589725255966,\n",
      "            0.0021677864715456963,\n",
      "            0.007791404612362385,\n",
      "            0.007126937620341778,\n",
      "            0.007228357717394829,\n",
      "            0.09992186725139618,\n",
      "            0.01858648844063282,\n",
      "            0.07342641800642014,\n",
      "            0.04258489981293678,\n",
      "            0.07545389980077744,\n",
      "            0.05607406795024872,\n",
      "            0.00988832674920559,\n",
      "            0.015340102836489677,\n",
      "            0.0014388462295755744,\n",
      "            0.0069213625974953175,\n",
      "            0.009875076822936535,\n",
      "            0.12176883220672607,\n",
      "            0.014499211683869362,\n",
      "            0.03660384565591812,\n",
      "            0.0024552608374506235,\n",
      "            0.0015000015264376998,\n",
      "            0.007959687151014805,\n",
      "            0.17433063685894012\n",
      "        ],\n",
      "        \"6.acc\": [\n",
      "            99.2\n",
      "        ],\n",
      "        \"7.loss\": [\n",
      "            0.00438105221837759,\n",
      "            0.10936161130666733,\n",
      "            0.008834537118673325,\n",
      "            0.015834856778383255,\n",
      "            0.04759373515844345,\n",
      "            0.0065819378942251205,\n",
      "            0.025143969804048538,\n",
      "            0.06153372675180435,\n",
      "            0.0654066950082779,\n",
      "            0.003580142045393586,\n",
      "            0.0047128996811807156,\n",
      "            0.023714061826467514,\n",
      "            0.022013764828443527,\n",
      "            0.001093170139938593,\n",
      "            0.005399106070399284,\n",
      "            0.006186814047396183,\n",
      "            0.03459085151553154,\n",
      "            0.012728105299174786,\n",
      "            0.005272988695651293,\n",
      "            0.008540370501577854,\n",
      "            0.0012558591552078724,\n",
      "            0.10579581558704376,\n",
      "            0.0005276090814732015,\n",
      "            0.016418831422924995,\n",
      "            0.003232811111956835,\n",
      "            0.0023183885496109724,\n",
      "            0.04973794147372246,\n",
      "            0.005571727175265551,\n",
      "            0.03556578606367111,\n",
      "            0.0010506497928872705,\n",
      "            0.005891168024390936,\n",
      "            0.027479127049446106,\n",
      "            0.0012089451774954796,\n",
      "            0.007914514280855656,\n",
      "            0.003590390784665942,\n",
      "            0.0008532837964594364,\n",
      "            0.03023790568113327,\n",
      "            0.06436186283826828,\n",
      "            0.0034449121449142694,\n",
      "            0.016151610761880875,\n",
      "            0.015470344573259354,\n",
      "            0.010064272210001945,\n",
      "            0.00507038738578558,\n",
      "            0.012204568833112717,\n",
      "            0.011655758135020733,\n",
      "            0.0253510233014822,\n",
      "            0.004483687691390514,\n",
      "            0.051503878086805344,\n",
      "            0.02446749061346054,\n",
      "            0.002385286847129464,\n",
      "            0.003200434148311615,\n",
      "            0.11008375883102417,\n",
      "            0.019260693341493607,\n",
      "            0.023553624749183655,\n",
      "            0.029022306203842163,\n",
      "            0.054417721927165985,\n",
      "            0.0006158809992484748,\n",
      "            0.023076120764017105,\n",
      "            0.043442755937576294,\n",
      "            0.010066887363791466,\n",
      "            0.0007696851389482617,\n",
      "            0.06527525186538696,\n",
      "            0.013823411427438259,\n",
      "            0.0026273115072399378,\n",
      "            0.07546854019165039,\n",
      "            0.022731546312570572,\n",
      "            0.006927750539034605,\n",
      "            0.009601492434740067,\n",
      "            0.0033608698286116123,\n",
      "            0.01889960654079914,\n",
      "            0.08417858928442001,\n",
      "            0.001823124010115862,\n",
      "            0.0044945236295461655,\n",
      "            0.05464494228363037,\n",
      "            0.0006585344090126455,\n",
      "            0.022231370210647583,\n",
      "            0.06670963019132614,\n",
      "            0.0031334180384874344,\n",
      "            0.007778065279126167,\n",
      "            0.013653830625116825,\n",
      "            0.0015868962509557605,\n",
      "            0.019501611590385437,\n",
      "            0.04539699852466583,\n",
      "            0.007115329150110483,\n",
      "            0.0556989349424839,\n",
      "            0.013618338853120804,\n",
      "            0.01444526482373476,\n",
      "            0.0051681119948625565,\n",
      "            0.010046839714050293,\n",
      "            0.03203171491622925,\n",
      "            0.11039647459983826,\n",
      "            0.03703860193490982,\n",
      "            0.007775906939059496,\n",
      "            0.011197060346603394\n",
      "        ],\n",
      "        \"7.acc\": [\n",
      "            99.1\n",
      "        ],\n",
      "        \"8.loss\": [\n",
      "            0.018218940123915672,\n",
      "            0.05106981843709946,\n",
      "            0.006560392677783966,\n",
      "            0.010550611652433872,\n",
      "            0.0563201867043972,\n",
      "            0.007186288479715586,\n",
      "            0.02815665863454342,\n",
      "            0.017175287008285522,\n",
      "            0.012596208602190018,\n",
      "            0.04794095829129219,\n",
      "            0.003087121993303299,\n",
      "            0.004034766927361488,\n",
      "            0.04904156178236008,\n",
      "            0.012373674660921097,\n",
      "            0.0016612975159659982,\n",
      "            0.14548105001449585,\n",
      "            0.10839496552944183,\n",
      "            0.004681256599724293,\n",
      "            0.005964556243270636,\n",
      "            0.003253570757806301,\n",
      "            0.0085988724604249,\n",
      "            0.002227832330390811,\n",
      "            0.0027477305848151445,\n",
      "            0.03154962137341499,\n",
      "            0.1517058163881302,\n",
      "            0.0023244477342814207,\n",
      "            0.04426250979304314,\n",
      "            0.0735769048333168,\n",
      "            0.05370679125189781,\n",
      "            0.02351083792746067,\n",
      "            0.008152073249220848,\n",
      "            0.014761989936232567,\n",
      "            0.033979274332523346,\n",
      "            0.03144242614507675,\n",
      "            0.006058153230696917,\n",
      "            0.0022375918924808502,\n",
      "            0.0030980368610471487,\n",
      "            0.002949977060779929,\n",
      "            0.028720872476696968,\n",
      "            0.011115694418549538,\n",
      "            0.022183571010828018,\n",
      "            0.13538570702075958,\n",
      "            0.012965141795575619,\n",
      "            0.005338708870112896,\n",
      "            0.061118509620428085,\n",
      "            0.008256850764155388,\n",
      "            0.013835044577717781,\n",
      "            0.08830849826335907,\n",
      "            0.00615340331569314,\n",
      "            0.011648537591099739,\n",
      "            0.03485281020402908,\n",
      "            0.12667612731456757,\n",
      "            0.0024907283950597048,\n",
      "            0.022527894005179405,\n",
      "            0.016698747873306274,\n",
      "            0.11120127141475677,\n",
      "            0.046433694660663605,\n",
      "            0.07962309569120407,\n",
      "            0.0040680463425815105,\n",
      "            0.007113459520041943,\n",
      "            0.0009367777383886278,\n",
      "            0.01935630291700363,\n",
      "            0.0006618392071686685,\n",
      "            0.010042821988463402,\n",
      "            0.012449320405721664,\n",
      "            0.05363794416189194,\n",
      "            0.00823290180414915,\n",
      "            0.018348969519138336,\n",
      "            0.07616539299488068,\n",
      "            0.06383145600557327,\n",
      "            0.023800769820809364,\n",
      "            0.13914333283901215,\n",
      "            0.0039061331190168858,\n",
      "            0.020270247012376785,\n",
      "            0.030033420771360397,\n",
      "            0.18095065653324127,\n",
      "            0.001477012992836535,\n",
      "            0.013823910616338253,\n",
      "            0.03208274021744728,\n",
      "            0.06577258557081223,\n",
      "            0.005683432333171368,\n",
      "            0.0011673039989545941,\n",
      "            0.011600244790315628,\n",
      "            0.0002726759121287614,\n",
      "            0.0001381821057293564,\n",
      "            0.005295642651617527,\n",
      "            0.0217712614685297,\n",
      "            0.003048090497031808,\n",
      "            0.07612716406583786,\n",
      "            0.010281519964337349,\n",
      "            0.00646691769361496,\n",
      "            0.0033730079885572195,\n",
      "            0.01384588610380888,\n",
      "            0.019856570288538933\n",
      "        ],\n",
      "        \"8.acc\": [\n",
      "            99.08\n",
      "        ],\n",
      "        \"9.loss\": [\n",
      "            0.053474120795726776,\n",
      "            0.018413415178656578,\n",
      "            0.015107166022062302,\n",
      "            0.020750638097524643,\n",
      "            0.01819763146340847,\n",
      "            0.0042843203991651535,\n",
      "            0.005733026657253504,\n",
      "            0.007989444769918919,\n",
      "            0.04570775106549263,\n",
      "            0.006195082329213619,\n",
      "            0.0013361077290028334,\n",
      "            0.014984444715082645,\n",
      "            0.04554111883044243,\n",
      "            0.02939947135746479,\n",
      "            0.036845628172159195,\n",
      "            0.012819894589483738,\n",
      "            0.008788419887423515,\n",
      "            0.012601521797478199,\n",
      "            0.04177139326930046,\n",
      "            0.0018438983242958784,\n",
      "            0.011733808554708958,\n",
      "            0.02748042717576027,\n",
      "            0.004596458747982979,\n",
      "            0.007634790614247322,\n",
      "            0.0069887470453977585,\n",
      "            0.004015731159597635,\n",
      "            0.0092384098097682,\n",
      "            0.017802108079195023,\n",
      "            0.05096765235066414,\n",
      "            0.0038827648386359215,\n",
      "            0.0006138525204733014,\n",
      "            0.0012159174075350165,\n",
      "            0.00929317343980074,\n",
      "            0.03557330742478371,\n",
      "            0.0002131196961272508,\n",
      "            0.003694541985169053,\n",
      "            0.11060848832130432,\n",
      "            0.0034691665787249804,\n",
      "            0.00821249932050705,\n",
      "            0.016832787543535233,\n",
      "            0.015518197789788246,\n",
      "            0.014133485965430737,\n",
      "            0.015347415581345558,\n",
      "            0.15361955761909485,\n",
      "            0.0034809873905032873,\n",
      "            0.0255294069647789,\n",
      "            0.005450346041470766,\n",
      "            0.019294416531920433,\n",
      "            0.00853030476719141,\n",
      "            0.003566955216228962,\n",
      "            0.03866933658719063,\n",
      "            0.012355937622487545,\n",
      "            0.0037821787409484386,\n",
      "            0.0022804380860179663,\n",
      "            0.07538062334060669,\n",
      "            0.02580840513110161,\n",
      "            0.004051396157592535,\n",
      "            0.027094727382063866,\n",
      "            0.0018053506501019,\n",
      "            0.026592835783958435,\n",
      "            0.013839051127433777,\n",
      "            0.004571889992803335,\n",
      "            0.024787060916423798,\n",
      "            0.13835927844047546,\n",
      "            0.03139125555753708,\n",
      "            0.03879028558731079,\n",
      "            0.03727952763438225,\n",
      "            0.009397494606673717,\n",
      "            0.07431524246931076,\n",
      "            0.010226454585790634,\n",
      "            0.05268441513180733,\n",
      "            0.11583637446165085,\n",
      "            0.027389101684093475,\n",
      "            0.001344660995528102,\n",
      "            0.007830239832401276,\n",
      "            0.0025814499240368605,\n",
      "            0.036621980369091034,\n",
      "            0.05632193386554718,\n",
      "            0.06139681860804558,\n",
      "            0.00885671004652977,\n",
      "            0.015182457864284515,\n",
      "            0.005559891927987337,\n",
      "            0.0007741146837361157,\n",
      "            0.11474110186100006,\n",
      "            0.005727909505367279,\n",
      "            0.019233206287026405,\n",
      "            0.02251148410141468,\n",
      "            0.06766074895858765,\n",
      "            0.030437419191002846,\n",
      "            0.03257851302623749,\n",
      "            0.0007978092762641609,\n",
      "            0.014695796184241772,\n",
      "            0.020314479246735573,\n",
      "            0.008752989582717419\n",
      "        ],\n",
      "        \"9.acc\": [\n",
      "            99.15\n",
      "        ],\n",
      "        \"10.loss\": [\n",
      "            0.006389308720827103,\n",
      "            0.0160311758518219,\n",
      "            0.08166785538196564,\n",
      "            0.07573844492435455,\n",
      "            0.041674792766571045,\n",
      "            0.0048859696835279465,\n",
      "            0.0063161891885101795,\n",
      "            0.04921434447169304,\n",
      "            0.0638744905591011,\n",
      "            0.045095618814229965,\n",
      "            0.013200046494603157,\n",
      "            0.04567418247461319,\n",
      "            0.0011047067819163203,\n",
      "            0.0017535933293402195,\n",
      "            0.010114296339452267,\n",
      "            0.014839659444987774,\n",
      "            0.008585336618125439,\n",
      "            0.019855553284287453,\n",
      "            0.0015153465792536736,\n",
      "            0.046598389744758606,\n",
      "            0.048699408769607544,\n",
      "            0.014482901431620121,\n",
      "            0.014686435461044312,\n",
      "            0.00635121064260602,\n",
      "            0.006970657035708427,\n",
      "            0.0077649326995015144,\n",
      "            0.011448790319263935,\n",
      "            0.001410390599630773,\n",
      "            0.003289155662059784,\n",
      "            0.07752121239900589,\n",
      "            0.008158784359693527,\n",
      "            0.002034102799370885,\n",
      "            0.003424752736464143,\n",
      "            0.0004501010407693684,\n",
      "            0.0008400123915635049,\n",
      "            0.006530389655381441,\n",
      "            0.010454731993377209,\n",
      "            0.01428688783198595,\n",
      "            0.0009423723677173257,\n",
      "            0.010081365704536438,\n",
      "            0.02367984689772129,\n",
      "            0.005217793397605419,\n",
      "            0.13537685573101044,\n",
      "            0.0020121538545936346,\n",
      "            0.021231843158602715,\n",
      "            0.04204584285616875,\n",
      "            0.0023681405000388622,\n",
      "            0.01024059858173132,\n",
      "            0.01647200435400009,\n",
      "            0.00701370183378458,\n",
      "            0.03606472164392471,\n",
      "            0.001676500658504665,\n",
      "            0.005888521671295166,\n",
      "            0.008531179279088974,\n",
      "            0.01612170785665512,\n",
      "            0.0013706913450732827,\n",
      "            0.003030333435162902,\n",
      "            0.22326546907424927,\n",
      "            0.0017411193111911416,\n",
      "            0.004412710200995207,\n",
      "            0.013105122372508049,\n",
      "            0.009098364040255547,\n",
      "            0.005586611106991768,\n",
      "            0.0052937790751457214,\n",
      "            0.00614397507160902,\n",
      "            0.004071460105478764,\n",
      "            0.00426913145929575,\n",
      "            0.08276019245386124,\n",
      "            0.002606909954920411,\n",
      "            0.00647496385499835,\n",
      "            0.00012373749632388353,\n",
      "            0.006626529153436422,\n",
      "            0.017001312226057053,\n",
      "            0.05794959515333176,\n",
      "            0.07748555392026901,\n",
      "            0.008597818203270435,\n",
      "            0.30470260977745056,\n",
      "            0.002591540804132819,\n",
      "            0.03509927913546562,\n",
      "            0.0023100641556084156,\n",
      "            0.041951656341552734,\n",
      "            0.14621397852897644,\n",
      "            0.003271508729085326,\n",
      "            0.04953077435493469,\n",
      "            0.0047319140285253525,\n",
      "            0.04825188219547272,\n",
      "            0.06388939172029495,\n",
      "            0.009133105166256428,\n",
      "            0.016532905399799347,\n",
      "            0.003813087707385421,\n",
      "            8.58293060446158e-05,\n",
      "            0.01650765910744667,\n",
      "            0.027942227199673653,\n",
      "            0.00490756006911397\n",
      "        ],\n",
      "        \"10.acc\": [\n",
      "            99.14\n",
      "        ],\n",
      "        \"11.loss\": [\n",
      "            0.007454691454768181,\n",
      "            0.004708403255790472,\n",
      "            0.0039942520670592785,\n",
      "            0.025484900921583176,\n",
      "            0.003215418430045247,\n",
      "            0.15819264948368073,\n",
      "            0.008747925981879234,\n",
      "            0.07463797181844711,\n",
      "            0.0020031172316521406,\n",
      "            0.010277437046170235,\n",
      "            0.02525419555604458,\n",
      "            0.0020131717901676893,\n",
      "            0.00979095883667469,\n",
      "            0.007477695122361183,\n",
      "            0.03056968003511429,\n",
      "            0.006918010301887989,\n",
      "            0.008414983749389648,\n",
      "            0.0021740328520536423,\n",
      "            0.001430869335308671,\n",
      "            0.044407378882169724,\n",
      "            0.001775765442289412,\n",
      "            0.0006284091505222023,\n",
      "            0.015389427542686462,\n",
      "            0.05214809253811836,\n",
      "            0.007121690083295107,\n",
      "            0.020215539261698723,\n",
      "            0.006033528596162796,\n",
      "            0.0074984910897910595,\n",
      "            0.014929861761629581,\n",
      "            0.007788094691932201,\n",
      "            0.0030033935327082872,\n",
      "            0.008425256237387657,\n",
      "            0.024114379659295082,\n",
      "            0.019148793071508408,\n",
      "            0.028744526207447052,\n",
      "            0.008931277319788933,\n",
      "            0.004578957334160805,\n",
      "            0.0031536875758320093,\n",
      "            0.02629019133746624,\n",
      "            0.0018820022232830524,\n",
      "            0.04224129021167755,\n",
      "            0.008224056102335453,\n",
      "            0.01579660177230835,\n",
      "            0.005283399019390345,\n",
      "            0.0018329243175685406,\n",
      "            0.028695471584796906,\n",
      "            0.007786832749843597,\n",
      "            0.014498118311166763,\n",
      "            0.0006394238444045186,\n",
      "            0.025758011266589165,\n",
      "            0.07820775359869003,\n",
      "            0.0068540386855602264,\n",
      "            0.006319166626781225,\n",
      "            0.11016634106636047,\n",
      "            0.09624537825584412,\n",
      "            0.00042267321259714663,\n",
      "            0.023983225226402283,\n",
      "            0.006953494157642126,\n",
      "            0.014229380525648594,\n",
      "            0.008730445057153702,\n",
      "            0.144292950630188,\n",
      "            0.008818751201033592,\n",
      "            0.04505212604999542,\n",
      "            0.0034509736578911543,\n",
      "            0.013492574915289879,\n",
      "            0.011512408964335918,\n",
      "            0.004624269902706146,\n",
      "            0.04620139300823212,\n",
      "            0.019657336175441742,\n",
      "            0.0022767730988562107,\n",
      "            0.003719518892467022,\n",
      "            0.07043993473052979,\n",
      "            0.026421185582876205,\n",
      "            0.03823027387261391,\n",
      "            0.007518172729760408,\n",
      "            0.005712972022593021,\n",
      "            0.01806897111237049,\n",
      "            0.0009612515568733215,\n",
      "            0.004700705874711275,\n",
      "            0.020185062661767006,\n",
      "            0.01734602078795433,\n",
      "            0.008201795630156994,\n",
      "            0.0008482339326292276,\n",
      "            0.01297333650290966,\n",
      "            0.08698312938213348,\n",
      "            0.0012134624412283301,\n",
      "            0.044051095843315125,\n",
      "            0.0060014487244188786,\n",
      "            0.0009332697372883558,\n",
      "            0.04742415249347687,\n",
      "            0.005147693213075399,\n",
      "            0.002966536208987236,\n",
      "            0.022051231935620308,\n",
      "            0.06653910875320435\n",
      "        ],\n",
      "        \"11.acc\": [\n",
      "            99.16\n",
      "        ],\n",
      "        \"12.loss\": [\n",
      "            0.058512166142463684,\n",
      "            0.0030579660087823868,\n",
      "            0.21969842910766602,\n",
      "            0.04198632389307022,\n",
      "            0.010407986119389534,\n",
      "            0.053352124989032745,\n",
      "            0.0019826923962682486,\n",
      "            0.028511516749858856,\n",
      "            0.0728820189833641,\n",
      "            0.0026043120305985212,\n",
      "            0.01714928261935711,\n",
      "            0.0052375104278326035,\n",
      "            0.04281497374176979,\n",
      "            0.024286365136504173,\n",
      "            0.07094033062458038,\n",
      "            0.007533013354986906,\n",
      "            0.003629765473306179,\n",
      "            0.0027562142349779606,\n",
      "            0.00620146794244647,\n",
      "            0.005087768658995628,\n",
      "            0.012151548638939857,\n",
      "            0.0029588970355689526,\n",
      "            0.0014416677877306938,\n",
      "            0.0006211496074683964,\n",
      "            0.03246177360415459,\n",
      "            0.02420579455792904,\n",
      "            0.006417231634259224,\n",
      "            0.05049699544906616,\n",
      "            0.010321133770048618,\n",
      "            0.011431911960244179,\n",
      "            0.027799099683761597,\n",
      "            0.003861337900161743,\n",
      "            0.004093175753951073,\n",
      "            0.0014752615243196487,\n",
      "            0.03214924782514572,\n",
      "            0.00855141319334507,\n",
      "            0.009934421628713608,\n",
      "            0.07391400635242462,\n",
      "            0.024039853364229202,\n",
      "            0.05847488343715668,\n",
      "            0.027306409552693367,\n",
      "            0.037048108875751495,\n",
      "            0.007688323967158794,\n",
      "            0.002147899940609932,\n",
      "            0.03492841124534607,\n",
      "            0.0014116645324975252,\n",
      "            0.06407184898853302,\n",
      "            0.0065938257612288,\n",
      "            0.002366567961871624,\n",
      "            0.05031681805849075,\n",
      "            0.09436684101819992,\n",
      "            0.004171180538833141,\n",
      "            0.0047913906164467335,\n",
      "            0.056196145713329315,\n",
      "            0.0053124697878956795,\n",
      "            0.015086539089679718,\n",
      "            0.06340312957763672,\n",
      "            0.007065264508128166,\n",
      "            0.0001423773355782032,\n",
      "            0.0031458386220037937,\n",
      "            0.0038677151314914227,\n",
      "            0.02753245085477829,\n",
      "            0.01031697541475296,\n",
      "            0.030009111389517784,\n",
      "            0.00023454753682017326,\n",
      "            0.014314411208033562,\n",
      "            0.0017642219318076968,\n",
      "            0.05805575102567673,\n",
      "            0.0015521716559305787,\n",
      "            0.014021436683833599,\n",
      "            0.0004624469147529453,\n",
      "            0.004865677561610937,\n",
      "            0.019191760569810867,\n",
      "            0.0011965084122493863,\n",
      "            0.0007500441861338913,\n",
      "            0.04300979897379875,\n",
      "            0.06471342593431473,\n",
      "            0.021121690049767494,\n",
      "            0.008309060707688332,\n",
      "            0.08870535343885422,\n",
      "            0.04065343365073204,\n",
      "            0.0005183215835131705,\n",
      "            0.06552209705114365,\n",
      "            0.0026841808576136827,\n",
      "            0.10088256746530533,\n",
      "            0.014212741516530514,\n",
      "            0.009362488985061646,\n",
      "            0.04988466203212738,\n",
      "            0.006350373849272728,\n",
      "            0.06118934229016304,\n",
      "            0.036627866327762604,\n",
      "            0.005930277518928051,\n",
      "            0.10256365686655045,\n",
      "            0.021836835891008377\n",
      "        ],\n",
      "        \"12.acc\": [\n",
      "            99.21\n",
      "        ],\n",
      "        \"13.loss\": [\n",
      "            0.01702911965548992,\n",
      "            0.019484544172883034,\n",
      "            0.008838366717100143,\n",
      "            0.013087783008813858,\n",
      "            0.0021730551961809397,\n",
      "            0.07811447978019714,\n",
      "            0.0032080726232379675,\n",
      "            0.026568694040179253,\n",
      "            0.0009629001724533737,\n",
      "            0.0015212544240057468,\n",
      "            0.002080379519611597,\n",
      "            0.001198977930471301,\n",
      "            0.03185807913541794,\n",
      "            0.005184011999517679,\n",
      "            0.013425381854176521,\n",
      "            0.002486548386514187,\n",
      "            0.0036725662648677826,\n",
      "            0.0027284903917461634,\n",
      "            0.027402909472584724,\n",
      "            0.011249187402427197,\n",
      "            0.001961352303624153,\n",
      "            0.009960267692804337,\n",
      "            0.002288833726197481,\n",
      "            0.09939683973789215,\n",
      "            0.0009426103206351399,\n",
      "            0.0038578188978135586,\n",
      "            0.020194511860609055,\n",
      "            0.017250079661607742,\n",
      "            0.0026881892699748278,\n",
      "            0.05365185812115669,\n",
      "            0.035107485949993134,\n",
      "            0.02280048280954361,\n",
      "            0.028815217316150665,\n",
      "            0.019681427627801895,\n",
      "            0.004171706736087799,\n",
      "            0.010952135547995567,\n",
      "            0.03507710248231888,\n",
      "            0.057868920266628265,\n",
      "            0.0011682522017508745,\n",
      "            0.0019284267909824848,\n",
      "            0.003076357999816537,\n",
      "            0.0037835054099559784,\n",
      "            0.0023357588797807693,\n",
      "            0.06121550500392914,\n",
      "            0.0012623760849237442,\n",
      "            0.004865153227001429,\n",
      "            0.004937084391713142,\n",
      "            0.0024516768753528595,\n",
      "            0.0022251568734645844,\n",
      "            0.0013530431315302849,\n",
      "            0.0026671162340790033,\n",
      "            0.024671444669365883,\n",
      "            0.00511068943887949,\n",
      "            0.004219407215714455,\n",
      "            0.0019712988287210464,\n",
      "            0.10877640545368195,\n",
      "            0.01471211202442646,\n",
      "            0.005159445106983185,\n",
      "            0.008853076957166195,\n",
      "            0.10644487291574478,\n",
      "            0.006384809967130423,\n",
      "            0.0009186939569190145,\n",
      "            0.0009921167511492968,\n",
      "            0.0034218821674585342,\n",
      "            0.008281683549284935,\n",
      "            0.135898619890213,\n",
      "            0.0007978684734553099,\n",
      "            0.031217612326145172,\n",
      "            0.0028788133058696985,\n",
      "            0.012039175257086754,\n",
      "            0.004661903716623783,\n",
      "            0.010333242826163769,\n",
      "            0.007167093921452761,\n",
      "            0.01376526802778244,\n",
      "            0.0016338273417204618,\n",
      "            0.001433857250958681,\n",
      "            0.02852894552052021,\n",
      "            0.02262989990413189,\n",
      "            0.03589428588747978,\n",
      "            0.0110904760658741,\n",
      "            0.01358936820179224,\n",
      "            0.00323120248503983,\n",
      "            0.027925904840230942,\n",
      "            0.0023193685337901115,\n",
      "            0.039490073919296265,\n",
      "            0.0006677109631709754,\n",
      "            0.007358306087553501,\n",
      "            0.0017861383967101574,\n",
      "            0.0027033165097236633,\n",
      "            0.0070535591803491116,\n",
      "            0.05468272045254707,\n",
      "            0.004768137354403734,\n",
      "            0.08394476026296616,\n",
      "            0.012861855328083038\n",
      "        ],\n",
      "        \"13.acc\": [\n",
      "            99.15\n",
      "        ],\n",
      "        \"14.loss\": [\n",
      "            0.0013312610099092126,\n",
      "            0.004504740238189697,\n",
      "            0.00393007043749094,\n",
      "            0.020300909876823425,\n",
      "            0.006379747297614813,\n",
      "            0.06214628741145134,\n",
      "            0.004319517407566309,\n",
      "            0.005633621476590633,\n",
      "            0.012159801088273525,\n",
      "            0.03314035013318062,\n",
      "            0.00843054335564375,\n",
      "            0.0003173162112943828,\n",
      "            0.01492295227944851,\n",
      "            0.0045883082784712315,\n",
      "            0.004123296122997999,\n",
      "            0.030665451660752296,\n",
      "            6.945200584596023e-05,\n",
      "            0.012988272123038769,\n",
      "            0.1452063024044037,\n",
      "            0.021295858547091484,\n",
      "            0.004064612090587616,\n",
      "            0.033575937151908875,\n",
      "            0.00567564507946372,\n",
      "            0.012211290188133717,\n",
      "            0.04027881845831871,\n",
      "            0.013192227110266685,\n",
      "            0.0016817607684060931,\n",
      "            0.05110368877649307,\n",
      "            0.007882481440901756,\n",
      "            0.00039132428355515003,\n",
      "            0.004560641944408417,\n",
      "            0.005952358245849609,\n",
      "            0.014700070954859257,\n",
      "            0.0040984745137393475,\n",
      "            0.062428854405879974,\n",
      "            0.08150963485240936,\n",
      "            0.023602548986673355,\n",
      "            0.015872564166784286,\n",
      "            0.0008477977244183421,\n",
      "            0.0018568953964859247,\n",
      "            0.025214366614818573,\n",
      "            0.005344882607460022,\n",
      "            0.0006267226999625564,\n",
      "            0.014900831505656242,\n",
      "            0.0029074724297970533,\n",
      "            0.0689067542552948,\n",
      "            0.010401166044175625,\n",
      "            0.1300213485956192,\n",
      "            0.022416386753320694,\n",
      "            0.0038741000462323427,\n",
      "            0.00038843098445795476,\n",
      "            0.002463008277118206,\n",
      "            0.0056472825817763805,\n",
      "            0.0009373397915624082,\n",
      "            0.0005537641700357199,\n",
      "            0.044445645064115524,\n",
      "            0.00948374718427658,\n",
      "            0.010156200267374516,\n",
      "            0.0021289056167006493,\n",
      "            0.01011698693037033,\n",
      "            0.11006881296634674,\n",
      "            0.002870062831789255,\n",
      "            0.0005093928193673491,\n",
      "            0.013262657448649406,\n",
      "            0.0010767634958028793,\n",
      "            0.049193765968084335,\n",
      "            0.12480461597442627,\n",
      "            0.09954230487346649,\n",
      "            0.04127560928463936,\n",
      "            0.0027352580800652504,\n",
      "            0.015064082108438015,\n",
      "            0.03327260538935661,\n",
      "            0.00016223017883021384,\n",
      "            0.037054963409900665,\n",
      "            0.0016416884027421474,\n",
      "            0.0015344073763117194,\n",
      "            0.07857474684715271,\n",
      "            0.04730302840471268,\n",
      "            0.031846486032009125,\n",
      "            0.04129190370440483,\n",
      "            0.16216503083705902,\n",
      "            0.0005296019953675568,\n",
      "            0.002167815575376153,\n",
      "            0.0048575927503407,\n",
      "            0.003200730076059699,\n",
      "            0.02105427160859108,\n",
      "            0.04207133129239082,\n",
      "            0.013169948011636734,\n",
      "            0.0021662607323378325,\n",
      "            0.0015299360966309905,\n",
      "            0.03001408837735653,\n",
      "            0.0011735359439626336,\n",
      "            0.020069994032382965,\n",
      "            0.03145855665206909\n",
      "        ],\n",
      "        \"14.acc\": [\n",
      "            99.13\n",
      "        ]\n",
      "    }\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat .replay.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c5ae1-d2bd-412c-beec-e833bc785454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2225f516-4e6f-423f-b750-7cfca3116c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
